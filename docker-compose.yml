version: "3.8"

services:
  flask-api:
    # build allows for custom docker images, using a dockerfile in this case in backend/app
    build: ./backend/app
    # set on port 5000
    ports:
      - "5000:5000"
    env_file:
      - .env
    # flask service depends on kafka to start, to handle streaming data
    depends_on:
      - kafka
  
  zookeeper:
    # build from bitnami zookeeper image
    image: bitnami/zookeeper:3.8
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
  
  kafka:
    image: bitnami/kafka:3.5
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper
  
  spark:
    build: ./spark_processing
    container_name: spark
    environment:
      - SPARK_MODE=master
    env_file:
      - .env
    depends_on:
      - kafka
    volumes:
      - ./spark_processing:/opt/spark-app
    command: bash -c "sleep 10 && spark-submit 
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
      /opt/spark-app/spark_consumer.py"
  
  mongodb:
    image: mongo
    container_name: mongodb_container
    ports:
      - "27017:27017"
    # volumes attaches persistent storage to mongodb containe, ensuring the data remains accessible 
    volumes:
      - "C:/Users/Andee Chong/Documents/Monash (Personal)/esp32-flask/mongo_data:/data/db"
    depends_on:
      - spark

